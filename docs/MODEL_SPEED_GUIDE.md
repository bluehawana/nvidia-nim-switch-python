# ğŸš€ Model Speed Guide

## Understanding Model Performance

Not all AI models are created equal! Some respond in seconds, others take much longer. This guide helps you choose the right model for your needs.

---

## ğŸ¯ Speed Categories

### âš¡ Fast Models (< 10B parameters)
**Response Time**: 1-3 seconds  
**Best For**: Quick tasks, rapid iteration, testing

**Examples**:
- `meta/llama-3.1-8b-instruct` - Very fast, good quality
- `google/gemma-2-9b-it` - Fast and efficient
- `microsoft/phi-3-medium-4k-instruct` - Compact and quick
- `deepseek-ai/deepseek-coder-6.7b-instruct` - Fast coding model

**Use When**:
- âœ… You need quick responses
- âœ… Testing and prototyping
- âœ… Simple questions
- âœ… Code completion
- âœ… Quick translations

---

### ğŸš€ Medium Models (10B-70B parameters)
**Response Time**: 3-8 seconds  
**Best For**: Balanced performance and quality

**Examples**:
- `meta/llama-3.1-70b-instruct` - Great balance
- `nvidia/llama-3.1-nemotron-70b-instruct` - NVIDIA optimized
- `qwen/qwq-32b-preview` - Good reasoning
- `deepseek-ai/deepseek-r1-distill-qwen-32b` - Reasoning model

**Use When**:
- âœ… You need good quality responses
- âœ… Complex questions
- âœ… Code generation
- âœ… Analysis tasks
- âœ… Balanced speed/quality

---

### ğŸ¢ Slow Models (> 70B parameters)
**Response Time**: 10-30+ seconds  
**Best For**: Maximum quality, complex reasoning

**Examples**:
- `deepseek-ai/deepseek-v3.1` - Excellent reasoning
- `qwen/qwen3-coder-480b-a35b-instruct` - Best coding quality
- `deepseek-ai/deepseek-r1` - Advanced reasoning

**Use When**:
- âœ… You need the best quality
- âœ… Complex reasoning tasks
- âœ… Critical code generation
- âœ… In-depth analysis
- âœ… You can wait for results

---

## ğŸ“Š Speed vs Quality Trade-off

```
Fast (âš¡)     Medium (ğŸš€)    Slow (ğŸ¢)
â”‚              â”‚              â”‚
â”‚              â”‚              â”‚
Speed â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Quality â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
```

---

## ğŸ¯ How to Use Speed Indicators

### In the Web Interface

1. **Sort by Speed**
   - Click "Sort by: Speed"
   - Fastest models appear first
   - Perfect for finding quick models

2. **Filter by Speed**
   - Select "âš¡ Fast Only" to see only fast models
   - Select "ğŸš€ Medium+" to see fast and medium models
   - Select "ğŸ¢ All Speeds" to see everything

3. **Visual Indicators**
   - Each model shows a speed badge
   - âš¡ = Fast (green)
   - ğŸš€ = Medium (orange)
   - ğŸ¢ = Slow (red)

4. **Size Badges**
   - Shows parameter count (8B, 70B, etc.)
   - Smaller = Faster
   - Larger = Better quality

---

## ğŸ’¡ Recommendations by Use Case

### For Coding
```bash
# Fast iteration
nim-switch meta/llama-3.1-8b-instruct

# Balanced
nim-switch nvidia/llama-3.1-nemotron-70b-instruct

# Best quality
nim-switch qwen/qwen3-coder-480b-a35b-instruct
```

### For Reasoning
```bash
# Quick reasoning
nim-switch qwen/qwq-32b-preview

# Best reasoning
nim-switch deepseek-ai/deepseek-v3.1
```

### For General Chat
```bash
# Fast responses
nim-switch meta/llama-3.1-8b-instruct

# Balanced
nim-switch meta/llama-3.1-70b-instruct
```

---

## ğŸ”„ Quick Switching Strategy

### Start Fast, Go Slow When Needed

```bash
# Start with fast model for exploration
nim-switch meta/llama-3.1-8b-instruct

# Switch to medium for better quality
nim-switch nvidia/llama-3.1-nemotron-70b-instruct

# Switch to slow for critical tasks
nim-switch deepseek-ai/deepseek-v3.1
```

---

## ğŸ“ˆ Performance Tips

### 1. Use Fast Models for:
- Initial exploration
- Quick questions
- Testing prompts
- Rapid iteration
- Simple tasks

### 2. Use Medium Models for:
- Production code
- Complex questions
- Analysis
- Most daily tasks

### 3. Use Slow Models for:
- Critical decisions
- Complex reasoning
- Best quality output
- Final production code

---

## ğŸ¯ Real-World Example

### Scenario: Building a Feature

```bash
# Phase 1: Planning (use fast model)
nim-switch meta/llama-3.1-8b-instruct
# Ask: "What's the best approach for this feature?"
# Response time: 2 seconds âš¡

# Phase 2: Implementation (use medium model)
nim-switch nvidia/llama-3.1-nemotron-70b-instruct
# Ask: "Write the implementation"
# Response time: 5 seconds ğŸš€

# Phase 3: Review (use slow model)
nim-switch deepseek-ai/deepseek-v3.1
# Ask: "Review this code for issues"
# Response time: 15 seconds ğŸ¢
# But highest quality!
```

---

## ğŸ” How Speed is Determined

Speed is based on:
1. **Parameter Count** - Fewer parameters = Faster
2. **Architecture** - Some architectures are optimized
3. **Model Type** - Distilled models are faster
4. **NVIDIA Optimization** - Some models are optimized for NIM

---

## âš™ï¸ Technical Details

### Parameter Counts
- **1B-10B**: âš¡ Fast (1-3 seconds)
- **10B-70B**: ğŸš€ Medium (3-8 seconds)
- **70B+**: ğŸ¢ Slow (10-30+ seconds)

### Special Cases
- **Distilled models**: Usually faster than base models
- **Quantized models**: Faster with slight quality trade-off
- **NVIDIA optimized**: Faster on NIM infrastructure

---

## ğŸ‰ Summary

- âš¡ **Fast**: Quick responses, good for iteration
- ğŸš€ **Medium**: Balanced speed and quality
- ğŸ¢ **Slow**: Best quality, worth the wait

**Pro Tip**: Start fast, switch to slow only when you need maximum quality!

---

*Last updated: February 2, 2026*
